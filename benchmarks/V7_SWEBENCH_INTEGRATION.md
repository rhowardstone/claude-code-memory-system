# V7 Memory System Integration with SWE-Bench-CL

## Overview

Successfully integrated our V7 memory system with SWE-Bench-CL, a continual learning benchmark for coding agents that tests memory retention across 273 chronological coding tasks.

## What is SWE-Bench-CL?

**SWE-Bench-CL** (Continual Learning for Coding Agents) is a benchmark explicitly designed to test memory retention across coding sessions:

- **273 tasks** across 8 repositories (django, sympy, sphinx, matplotlib, scikit-learn, astropy, xarray, pytest)
- **Chronological ordering**: Tasks ordered by creation date to simulate real development
- **Curriculum learning**: Tasks grouped by difficulty (easy ‚Üí hard)
- **Dependency awareness**: Tasks with file overlap test knowledge transfer

**Key Metrics:**
- Success rate
- Forgetting rate (performance on previously solved tasks)
- Forward/backward transfer
- Tool use efficiency
- CL-Score (combined metric)

**Paper**: https://arxiv.org/abs/2507.00014
**Repository**: https://github.com/thomasjoshi/agents-never-forget

---

## Why SWE-Bench-CL is Perfect for Our V7 System

1. **Explicitly tests memory retention** - the benchmark is BUILT to measure memory across sessions
2. **Built-in baseline comparison** - they provide FAISS-based memory implementation
3. **Real coding tasks** - actual GitHub issues, not synthetic benchmarks
4. **Standardized metrics** - forgetting rate, transfer learning, success rate
5. **Published results** - can compare against their baselines

This is exactly what we need to validate our V7 memory system!

---

## V7 Advantages Over Their FAISS System

### Their System (eval_v3_swe-agent):
- FAISS vector store for semantic memory
- Similarity-only retrieval (no task-context awareness)
- Fixed top-K retrieval (always returns K memories)
- Basic embeddings (no contextual prefix)

### Our V7 System:
- ‚úÖ **Task-context aware scoring**: Boosts memories related to current task entities
- ‚úÖ **Knowledge graph**: Captures relationships between FILES, FUNCTIONS, BUGS, etc.
- ‚úÖ **Adaptive K retrieval**: Returns 0-20 memories based on quality (no spam)
- ‚úÖ **Contextual embeddings**: Prepends session/time/file context before embedding
- ‚úÖ **Importance scoring**: 0-30 importance scores, auto-prune low-value memories
- ‚úÖ **Multi-hop traversal**: Finds related entities 1-2 hops away in knowledge graph

---

## Integration Architecture

### V7MemoryAdapter (`v7_memory_adapter.py`)

**Drop-in replacement** for their `SemanticMemory` class with identical interface:

```python
class V7MemoryAdapter:
    def add_entry(task_id, sequence_id, content, success, metadata):
        """Store task experience with V7 enhancements"""
        # - Extract entities (FILES, FUNCTIONS, etc.)
        # - Compute importance score (0-30)
        # - Generate contextual embedding
        # - Store in ChromaDB

    def retrieve_relevant(query, sequence_id_filter=None, num_results=None):
        """Retrieve with task-context awareness"""
        # - Query with contextual embedding
        # - Filter by repository if specified
        # - Apply adaptive K (quality-based filtering)
        # - Return top matches with similarity scores

    def clear():
        """Clear all memories"""
```

**Key Features:**
- Uses nomic-embed-text-v1.5 (768-dim, 8192 token context)
- Stores to ChromaDB with full V7 metadata
- Contextual embeddings: `"Session {id} at {time}. Repository: {repo}. {summary}"`
- Compatible with their evaluation framework

---

## Files Created

### 1. `v7_memory_adapter.py` (Tested ‚úÖ)
- 400+ lines
- Implements `V7MemoryAdapter` and `V7MemorySystem` classes
- Drop-in replacement for their FAISS memory
- Includes test suite (passes!)

### 2. `run_swebench_cl_with_v7.py`
- Simplified integration runner
- Demonstrates how to use V7 adapter with SWE-Bench-CL
- Includes:
  - Dataset loading
  - Memory-enabled vs baseline comparison
  - Pilot test mode (10 tasks)
  - Results tracking and export

**Usage:**
```bash
# Run pilot with V7 memory
python3 run_swebench_cl_with_v7.py --use-memory --num-tasks 10

# Run baseline without memory
python3 run_swebench_cl_with_v7.py --no-memory --num-tasks 10

# Run full benchmark (273 tasks)
python3 run_swebench_cl_with_v7.py --use-memory --num-tasks 273
```

---

## Integration Status

‚úÖ **COMPLETE**:
1. V7 memory adapter implemented
2. Interface compatibility verified
3. Test suite passes
4. Integration script created
5. SWE-Bench-CL dataset downloaded and ready

‚è≥ **NEXT STEPS**:
1. Run pilot: 10 tasks WITHOUT memory (baseline)
2. Run pilot: 10 tasks WITH V7 memory
3. Compare success rates, memory usage
4. Analyze forgetting rate, transfer learning
5. Scale to full benchmark (273 tasks)

---

## How to Run Evaluation

### Quick Start (Pilot Test)

```bash
cd /atb-data/claude-code/claude-code/claude-memory-system/benchmarks

# 1. Run baseline (no memory)
python3 run_swebench_cl_with_v7.py \
    --no-memory \
    --num-tasks 10 \
    --output pilot_baseline.json

# 2. Run with V7 memory
python3 run_swebench_cl_with_v7.py \
    --use-memory \
    --num-tasks 10 \
    --output pilot_v7.json

# 3. Compare results
python3 -c "
import json
with open('pilot_baseline.json') as f:
    baseline = json.load(f)
with open('pilot_v7.json') as f:
    v7 = json.load(f)

print(f'Baseline success rate: {baseline[\"success_rate\"]:.1%}')
print(f'V7 success rate: {v7[\"success_rate\"]:.1%}')
print(f'Improvement: {(v7[\"success_rate\"] - baseline[\"success_rate\"]):.1%}')
"
```

### Full Benchmark

For the full 273 tasks, you would integrate directly with their `eval_v3_swe-agent/eval_procedure.py`:

1. Replace their `SemanticMemory` class with our `V7MemoryAdapter`
2. Update imports to use our adapter
3. Run their full evaluation pipeline
4. Compare against their published baselines

---

## Expected Results

Based on Anthropic's Contextual Retrieval findings (35% reduction in retrieval failures), we expect:

- **Better success rate**: V7 should outperform baseline by 10-35%
- **Lower forgetting rate**: Contextual embeddings help temporal queries
- **Better transfer learning**: Knowledge graph captures cross-file dependencies
- **Higher memory efficiency**: Adaptive K reduces irrelevant memory spam

---

## Technical Details

### Memory Schema (ChromaDB Metadata)

```python
{
    "session_id": "django__django-11001",  # Task ID
    "sequence_id": "django/django",        # Repository
    "timestamp": "2025-10-15T19:10:30.123",
    "importance_score": 15.0,               # 0-30
    "importance_category": "high",          # low/medium/high/critical
    "intent": "Solve issue in django/django: Fix auth bug",
    "action": "Added validation check",
    "outcome": "Tests passed",
    "chunk_index": 0,
    "has_code": true,
    "has_files": true,
    "success": true,
    "artifacts": "{...}"                    # JSON: files, functions, errors
}
```

### Contextual Embedding Format

```
Session abc12345 at 2025-10-15 19:10. Repository: django/django.
Solve issue in django/django: Fix auth bug ‚Üí Added validation check ‚Üí Tests passed
```

This format enables:
- **Temporal queries**: "work from yesterday"
- **Repository filtering**: "changes to django"
- **Session tracking**: "session abc123"

---

## Comparison with Industry Standards

| Feature | SWE-Bench-CL FAISS | Our V7 | Anthropic Contextual Retrieval |
|---------|-------------------|--------|-------------------------------|
| **Contextual Embeddings** | ‚ùå | ‚úÖ 20-30 token prefix | ‚úÖ 50-100 token prefix |
| **Task-Context Scoring** | ‚ùå | ‚úÖ Knowledge graph + PageRank | ‚ùå |
| **Adaptive K** | ‚ùå (fixed K) | ‚úÖ 0-20 based on quality | ‚ùå |
| **Importance Scoring** | ‚ùå | ‚úÖ 0-30 scale | ‚ùå |
| **Reranking** | ‚ùå | ‚ùå (local-first) | ‚úÖ Claude API |
| **Open Source** | ‚úÖ | ‚úÖ | ‚ùå (proprietary) |

Our V7 system is the **only open-source implementation** with both contextual embeddings AND task-context awareness!

---

## Repository Structure

```
benchmarks/
‚îú‚îÄ‚îÄ v7_memory_adapter.py              # V7 adapter (tested ‚úÖ)
‚îú‚îÄ‚îÄ run_swebench_cl_with_v7.py       # Integration runner
‚îú‚îÄ‚îÄ V7_SWEBENCH_INTEGRATION.md       # This file
‚îú‚îÄ‚îÄ agents-never-forget/              # SWE-Bench-CL repo
‚îÇ   ‚îú‚îÄ‚îÄ data/SWE-Bench-CL-Curriculum.json
‚îÇ   ‚îú‚îÄ‚îÄ eval_v3_swe-agent/            # Their evaluation framework
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ quick_start_swe_bench_cl.sh      # Setup script
```

---

## Citations

**SWE-Bench-CL:**
- Paper: [Agents Never Forget](https://arxiv.org/abs/2507.00014)
- Authors: Thomas Joshi, Shayan Chowdhury, Fatih Uysal
- Published: July 2024

**Anthropic Contextual Retrieval:**
- [Announcement](https://www.anthropic.com/news/contextual-retrieval)
- Published: September 2024
- Key finding: 35% reduction in retrieval failures with contextual embeddings

**Our V7 System:**
- Builds on: LexRank (graph centrality), Nomic Embeddings (MTEB top-10)
- Innovations: Task-context scoring, adaptive K, contextual embeddings
- Version: 7.0.0 (October 2025)

---

## Next Steps

1. **Run Pilot Tests** (10 tasks each):
   - Baseline: No memory
   - V7: With our memory system
   - Compare: Success rate, memory usage

2. **Analyze Results**:
   - Success rate improvement
   - Forgetting rate (re-test previously solved tasks)
   - Transfer learning (cross-repository)
   - Memory efficiency (adaptive K impact)

3. **Scale Up**:
   - Full benchmark (273 tasks)
   - Parallelize with 50+ workers (190 CPUs available)
   - Complete in hours instead of days

4. **Document Findings**:
   - Compare against their FAISS baseline
   - Validate Anthropic's 35% improvement claim
   - Publish results as evidence for V7 effectiveness

---

## Status

üéâ **Integration Complete!**

The V7 memory system is now ready to be tested against the SWE-Bench-CL benchmark. All components are in place:
- ‚úÖ Adapter implemented and tested
- ‚úÖ Integration script created
- ‚úÖ Dataset downloaded and ready
- ‚úÖ Evaluation pipeline understood

**Ready to run pilot tests and compare results!**
